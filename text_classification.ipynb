{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_classification",
      "provenance": [],
      "collapsed_sections": [
        "co19qCLP3Sis",
        "whPRbBNbIrIl",
        "-P3xD98R3eoQ",
        "QMfcudgK3jgH",
        "545PP3o8IrJV",
        "E1MqxBJQ4N2B"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shikha-aggarwal/researchpaperlikes/blob/main/text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co19qCLP3Sis"
      },
      "source": [
        "#### 1. Install and Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "source": [
        "!pip install datasets transformers pytorch-lightning --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ul82czF7JjdF"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    DistilBertTokenizer,\n",
        "    DistilBertForMultipleChoice,\n",
        "    AdamW,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import argparse\n",
        "import random\n",
        "import time\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import RandomSampler\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## Colab accessing Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "## navigate here to your main project folder\n",
        "## or place this file in the main folder.\n",
        "\n",
        "from datasets import load_dataset\n",
        "from ml_project.data_loaders import load_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "####2. Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rsm-EaDnKFp_"
      },
      "source": [
        "data_dir = './data'\n",
        "tags_list = load_data.load_tags(data_dir)\n",
        "article_tags_2d_list = load_data.load_article_tags(data_dir)\n",
        "user_items_2d_np = load_data.load_user_article_likes(data_dir)\n",
        "articles_df, user_article_likes_2d_np = load_data.load_articles_and_user_article_likes(data_dir)"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOydXR-mgfZb"
      },
      "source": [
        "articles_df[\"tags\"] = article_tags_2d_list"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "869WmizMRJWa"
      },
      "source": [
        "train_df, validate_df, test_df = np.split(articles_df.sample(frac=1, random_state=42), \n",
        "                       [int(.6*len(articles_df)), int(.8*len(articles_df))])"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P3xD98R3eoQ"
      },
      "source": [
        "####3. Various constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_n2gkLHVVhuc"
      },
      "source": [
        "input_length = 100\n",
        "MODEL_CHECKPOINT = 'distilbert-base-uncased'"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMfcudgK3jgH"
      },
      "source": [
        "#### 4. Dataset class for trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nkigcuGQued"
      },
      "source": [
        "class ArticlesDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, input_length):\n",
        "        self.pandas_df = df\n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained()         \n",
        "        self.input_length = input_length\n",
        "  \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pandas_df)\n",
        "    \n",
        "\n",
        "    def clean_text(self, text):\n",
        "        text = text.replace('\\n','')\n",
        "        text = text.replace('``', '')\n",
        "        text = text.replace('\"', '')\n",
        "        \n",
        "        return text\n",
        "    \n",
        "\n",
        "    def convert_to_features(self, example_batch):\n",
        "        sentence_1 = self.clean_text(example_batch['raw_title'] + \\\n",
        "                                     example_batch['raw_abstract'])  \n",
        "        source = self.tokenizer.batch_encode_plus([sentence_1], \n",
        "                                                  max_length=self.input_length,\n",
        "                                                  padding='max_length',\n",
        "                                                  truncation=True,\n",
        "                                                  return_tensors=\"pt\")\n",
        "        \n",
        "        return source\n",
        "  \n",
        "  \n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "\n",
        "        num_classes = len(tags_list)\n",
        "        source = self.convert_to_features(self.pandas_df.iloc[index])\n",
        "        \n",
        "        source_ids = source[\"input_ids\"].squeeze()\n",
        "        src_mask    = source[\"attention_mask\"].squeeze()\n",
        "        tags = self.pandas_df.iloc[index]['tags']\n",
        "        indices = [0] * len(tags_list)\n",
        "\n",
        "        ## not optimal\n",
        "        for i in range(len(tags)):\n",
        "            if tags[i] in tags_list:\n",
        "                indices[tags_list.index(tags[i])] = 1\n",
        "        \n",
        "        return {\"input_ids\": source_ids, \"attention_mask\": src_mask, \"labels\": torch.tensor(indices)}"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "#### 5. PyTorch Lightning Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZwo62X8rO8g"
      },
      "source": [
        "class DistilbertFineTuner(pl.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super(DistilbertFineTuner, self).__init__()\n",
        "        self.save_hyperparameters(hparams)\n",
        "        self.hparams = hparams\n",
        "        self.model = DistilBertForMultipleChoice.from_pretrained(self.hparams.tokenizer_name_or_path)\n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained(self.hparams.tokenizer_name_or_path)\n",
        "        utoTokenizer.from_pretrained()\n",
        "        self.model_dir = self.hparams.model_dir\n",
        "        \n",
        "    def forward(self, input_ids, labels):\n",
        "        return self.model(\n",
        "            input_ids = input_ids,\n",
        "            labels = labels,\n",
        "            )\n",
        "        \n",
        "    def _step(self, batch):\n",
        "        labels = batch['labels']\n",
        "        ## set padding token label to = -100 so that it is ignored\n",
        "        labels[labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
        "        outputs = self.forward(\n",
        "            input_ids = batch['input_ids'],\n",
        "            labels = labels\n",
        "        )\n",
        "        loss = outputs[0]\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self._step(batch)\n",
        "        self.log('train_loss', loss, on_step = True, on_epoch = True, \n",
        "                 prog_bar = True, logger = True)\n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        train_dataset = ArticlesDataset(train_df, tokenizer, input_length = input_length)\n",
        "        sampler = RandomSampler(train_dataset)\n",
        "\n",
        "        dataloader = DataLoader(train_dataset,\n",
        "                                sampler = sampler,\n",
        "                                batch_size = self.hparams.train_batch_size,\n",
        "                                drop_last = True,\n",
        "                                num_workers = 2)\n",
        "        return dataloader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        validation_dataset = ArticlesDataset(validation_df, tokenizer, input_length = input_length)\n",
        "        sampler = RandomSampler(validation_dataset)\n",
        "\n",
        "        return DataLoader(validation_dataset,\n",
        "                          batch_size = self.hparams.eval_batch_size,\n",
        "                          sampler = sampler,\n",
        "                          num_workers = 2)\n",
        "    \n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        ## Set bias decay to zero\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": self.hparams.weight_decay,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, lr = self.hparams.learning_rate)\n",
        "        # decreasing learning rate (linear) after increasing in num_warmup_steps\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer = optimizer,\n",
        "            num_warmup_steps = self.hparams.warmup_steps,\n",
        "            num_training_steps = self.hparams.training_steps\n",
        "        )\n",
        "        self.lr_scheduler = scheduler\n",
        "        self.log('configured optimizer: ', optimizer)\n",
        "        return optimizer"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1MqxBJQ4N2B"
      },
      "source": [
        "#### 6. Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0WWmkX9s59N"
      },
      "source": [
        "saved_models_path = '/content/drive/MyDrive/Colab Notebooks/research paper/saved_models'\n",
        "args_dict = dict(\n",
        "    model_dir               = saved_models_path,\n",
        "    model_name_or_path      = 'distilbert-base-uncased',\n",
        "    tokenizer_name_or_path  = ,\n",
        "    num_labels              = len(tags_list),\n",
        "    learning_rate           = 1e-5,\n",
        "    weight_decay            = 0.0,\n",
        "    adam_epsilon            = 1e-8,\n",
        "    warmup_steps            = 0,\n",
        "    train_batch_size        = 1,\n",
        "    eval_batch_size         = 1,\n",
        "    num_train_epochs        = 2,\n",
        "    accumulate_grad_batches = 10,\n",
        "    training_steps          = 10000,\n",
        "    n_gpu                   = 1,\n",
        "    resume_from_checkpoint  = None,\n",
        "    val_check_interval      = 0.5, \n",
        "    early_stop_callback     = False,\n",
        "    num_workers             = 0,\n",
        "    fp_16                   = False, \n",
        "    opt_level               = 'O1',\n",
        "    max_grad_norm           = 1.0,\n",
        ")\n",
        "args = argparse.Namespace(**args_dict)\n",
        "logger = TensorBoardLogger(\"tb_logs\", version = 1, name = 'distilbert')\n",
        "\n",
        "pl.seed_everything(42)\n",
        "Distilbert_finetuner = DistilbertFineTuner(args)\n",
        "\n",
        "train_params = dict(\n",
        "    default_root_dir = args.model_dir,\n",
        "    accumulate_grad_batches = 1,\n",
        "    gpus = args.n_gpu,\n",
        "    max_epochs = args.num_train_epochs,\n",
        "    precision = 16 if args.fp_16 else 32,\n",
        "    amp_level = args.opt_level,\n",
        "    resume_from_checkpoint = args.resume_from_checkpoint,\n",
        "    gradient_clip_val = args.max_grad_norm,\n",
        "    val_check_interval = args.val_check_interval,\n",
        "    progress_bar_refresh_rate = 20,\n",
        "    logger = logger\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(**train_params)\n",
        "\n",
        "trainer.fit(Distilbert_finetuner)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-9haYVc4eQ_"
      },
      "source": [
        "#### TODO:\n",
        "\n",
        "- This model will give us per article classification across all tags.\n",
        "- When any article is added, we can run this model and store the corresponding classification vector.\n",
        "- For a particular user, we can take either the last paper accessed or mean of the papers accessed to construct a tag_vector which can be used to find the nearest matching articles."
      ]
    }
  ]
}